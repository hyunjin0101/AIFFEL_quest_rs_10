{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```bash\n",
    "### 트랜스포머를 활용한 챗봇 만들기\n",
    "```\n",
    "\n",
    "```python\n",
    "#라이브러리 import\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "```bash\n",
    "## Step 1. Data Loads\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요.\n",
    "\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "$ ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "```\n",
    "\n",
    "\n",
    "#데이터 불러오기\n",
    "path = os.getenv('HOME') + '/aiffel/songys_chatbot/ChatbotData.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = 50000\n",
    "print(MAX_SAMPLES)\n",
    "\n",
    "\n",
    "\n",
    "## Step 2. Data Preprocessing\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 \n",
    "전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "    inputs, outputs = [], []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        inputs.append(preprocess_sentence(row['Q']))\n",
    "        outputs.append(preprocess_sentence(row['A']))\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 of Questions :', len(questions))\n",
    "print('전체 샘플 수 of Answers :', len(answers))\n",
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))\n",
    "\n",
    "\n",
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다.\n",
    "하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 \n",
    "내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요.\n",
    "\n",
    "\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성.\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(questions + \n",
    "                                                                    answers, target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "\n",
    "첫번째 에러: tfds의 라이브러리 버젼이 업데이트 되면서 SubwordTextEncoder의 위치가 변경 되었거나 더이상 tfds.features.text 경로 아래에 있지 않기 때문에 발생하는 에러이므로, 예전에는 저 경로안에 있었지만 최신 버전으로 업데이트 되면서 다른 경로로 옮겨졌거나 혹은 tensorflow_text 같은 다른 라이브러리를 사용 해야함.\n",
    "\n",
    "\n",
    "\n",
    "# 예시 (tensorflow_text 설치 필요: pip install tensorflow-text)\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Wordpiece 모델 구축 (BertTokenizer 등 활용)\n",
    "# 실제 사용 시에는 vocab 파일 경로 지정 또는 build_from_corpus 와 유사한 학습 과정 필요\n",
    "# 예시: BertTokenizer 사용\n",
    "tokenizer = text.BertTokenizer(vocab_lookup_table='path/to/vocab.txt', lower_case=True)\n",
    "\n",
    "# 또는 Sentencepiece 사용 (Sentencepiece 모델 파일 필요)\n",
    "# tokenizer = text.SentencepieceTokenizer(model=model_path)\n",
    "\n",
    "# build_from_corpus와 유사한 기능은 tensorflow_text 자체에는 직접 없을 수 있고,\n",
    "# 사전 훈련된 모델을 사용하거나, 별도의 라이브러리(예: sentencepiece)로 vocab을 먼저 만들어야 할 수 있어요.\n",
    "# 만약 build_from_corpus 기능이 꼭 필요하다면 아래 2번 옵션을 고려해보세요.\n",
    "\n",
    "\n",
    "두번째 에러: 앞에서의 첫번째 에러와 같이 tfds의 라이브러리 버젼이 안되어서 앞에 에러에서의 제안과 같이\n",
    "tensorflow_text와 같은 라이브러리의 사용을 고려해 돌렸더니, 설치가 안되어 있어서 에러가 남.\n",
    "\n",
    "\n",
    "# tensorflow_text를 설치\n",
    "!pip install tensorflow_text\n",
    "\n",
    "\n",
    "# 예시 (tensorflow_text 설치 필요: pip install tensorflow-text)\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Wordpiece 모델 구축 (BertTokenizer 등 활용)\n",
    "# 실제 사용 시에는 vocab 파일 경로 지정 또는 build_from_corpus 와 유사한 학습 과정 필요\n",
    "# 예시: BertTokenizer 사용\n",
    "tokenizer = text.BertTokenizer(vocab_lookup_table='path/to/vocab.txt', lower_case=True)\n",
    "\n",
    "# 또는 Sentencepiece 사용 (Sentencepiece 모델 파일 필요)\n",
    "# tokenizer = text.SentencepieceTokenizer(model=model_path)\n",
    "\n",
    "# build_from_corpus와 유사한 기능은 tensorflow_text 자체에는 직접 없을 수 있고,\n",
    "# 사전 훈련된 모델을 사용하거나, 별도의 라이브러리(예: sentencepiece)로 vocab을 먼저 만들어야 할 수 있어요.\n",
    "# 만약 build_from_corpus 기능이 꼭 필요하다면 아래 2번 옵션을 고려해보세요.\n",
    "\n",
    "\n",
    "\n",
    "세번째 에러: 라이브러리들을 업데이트 해주면서 다른 패키지들과의 호환성 문제가 생김. undefined_symbol 에러 같은 경우에는 현재의 tensorflow_text 같은 특정버전의 abs1-py 라이브러리의 함수를 필요로 하는데, 현재 환경에 로드된 abs1-py 버전과 맞지 않아 오류가 발생함. 패키지 버전 충돌로 인해 불안정해진 환경 상태임.\n",
    "\n",
    "\n",
    "\n",
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))\n",
    "\n",
    "\n",
    "\n",
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이.\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "    def tokenize_and_filter(inputs, outputs):\n",
    "        tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "      return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "\n",
    "#라이브러리 import\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
