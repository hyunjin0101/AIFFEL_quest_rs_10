{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f74dae",
   "metadata": {},
   "source": [
    "# MAIN QUEST 01 Transformer GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6d915",
   "metadata": {},
   "source": [
    "# Transformer vs. GPT-1 변경점 요약\n",
    "\n",
    "기존 Transformer 모델과 GPT-1 모델의 주요 차이점 및 코드 수정 방향은 다음과 같습니다:\n",
    "\n",
    "1.  **아키텍처:**\n",
    "    * Transformer: Encoder-Decoder 구조를 가집니다. 번역과 같은 Sequence-to-Sequence 작업에 주로 사용됩니다.\n",
    "    * GPT-1: Decoder-only 구조를 가집니다. 주로 언어 모델링 및 생성 작업에 특화되어 있습니다.\n",
    "    * **수정:** 제공된 코드는 이미 `gpt` 함수 내에서 Decoder 구조를 사용하고 있어, 아키텍처의 큰 변경은 필요하지 않습니다. 하지만 데이터 입력 파이프라인이 Seq2Seq 형태(input/target 쌍)가 아닌, 언어 모델링 형태(다음 토큰 예측)로 수정되어야 합니다.\n",
    "\n",
    "2.  **입력 처리:**\n",
    "    * Transformer: Encoder 입력과 Decoder 입력 (Teacher Forcing 시 이전 타겟 시퀀스)을 받습니다.\n",
    "    * GPT-1 (Pre-training): 연속된 텍스트 시퀀스를 입력으로 받아 다음 토큰을 예측하는 언어 모델링 목표를 사용합니다.\n",
    "    * **수정:** 기존 코드는 질문(Q)과 답변(A)을 별도로 토큰화하고, `inputs`와 `dec_inputs`를 데이터셋으로 구성했습니다. GPT-1 언어 모델 학습을 위해, 데이터를 단일 시퀀스로 처리하고, 입력 시퀀스 (`inputs`)와 타겟 시퀀스 (`outputs`, 입력 시퀀스를 한 칸 민 것)로 구성된 데이터셋으로 변경해야 합니다.\n",
    "\n",
    "3.  **Attention Mask:**\n",
    "    * Transformer: Encoder Self-Attention, Decoder Masked Self-Attention, Encoder-Decoder Attention 사용.\n",
    "    * GPT-1: Masked Self-Attention (Look-ahead Mask)만 사용합니다.\n",
    "    * **수정:** 제공된 코드의 `create_look_ahead_mask` 함수와 `MultiHeadAttention` 내 마스크 적용 로직은 GPT-1 방식과 호환됩니다.\n",
    "\n",
    "4.  **Position Encoding:**\n",
    "    * Transformer (Original): Sinusoidal (sine and cosine) 방식 사용.\n",
    "    * GPT-1: 학습 가능한 Wording Embedding and Position Embedding 사용. \n",
    "    * **수정:** 제공된 코드는 이미 학습 가능한 Position Embedding (`tf.keras.layers.Embedding`)을 사용하고 있어, 수정이 필요 없음. 하지만 여기서, word embedding도 같이 사용. 따라서, 텍스트를 숫자로 변환 후 임베딩, 단어 위치 정보를 학습 가능한 벡터로 변환, 단어의미 + 위치정보를 더하여 최종 입력 벡터 생성.\n",
    "\n",
    "5.  **Feed-Forward Network Activation:**\n",
    "    * Transformer (Original): ReLU 사용.\n",
    "    * GPT-1: GELU (Gaussian Error Linear Unit) 사용.\n",
    "    * **수정:** `decoder_layer` 내의 Dense 레이어 활성화 함수를 ReLU에서 GELU로 변경해야 합니다.\n",
    "\n",
    "6.  **데이터셋 구성 및 학습:**\n",
    "    * **수정:** `tf.data.Dataset` 생성 부분을 수정하여, 모델이 예상하는 입력 형태 (`inputs`)와 타겟 (`outputs`)을 올바르게 제공해야 합니다. 현재 코드의 데이터셋 구조는 `ValueError: Missing data for input \"input_1\"` 오류의 원인입니다. 이를 언어 모델링에 맞게 `(입력 시퀀스, 타겟 시퀀스)` 튜플 형태로 수정합니다. (처음에 발생한 오류 부분 수정) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54a7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "# MAIN QUEST 01 Transformer GPT-1 (코드 시작)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time # 시간 측정을 위해 추가\n",
    "import tensorflow_datasets as tfds # 토크나이저를 위해 추가\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dc11c",
   "metadata": {},
   "source": [
    "## Step 1. Data Loads & Preprocessing & Utilize SubwordTextEncoder\n",
    "\n",
    "1. 데이터 로드\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요.\n",
    "\n",
    "𝑚𝑘𝑑𝑖𝑟−𝑝 /𝑎𝑖𝑓𝑓𝑒𝑙/𝑡𝑟𝑎𝑛𝑠𝑓𝑜𝑟𝑚𝑒𝑟𝑐ℎ𝑎𝑡𝑏𝑜𝑡/𝑑𝑎𝑡𝑎/\n",
    "ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "\n",
    "2. 데이터 전처리\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 \n",
    "전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n",
    "\n",
    "3. 단어 토큰화 과정 \n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다.\n",
    "하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 \n",
    "내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38bbe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 질문 수: 11823\n",
      "총 답변 수: 11823\n",
      "전처리 후 샘플 질문: 12시 땡 !\n",
      "전처리 후 샘플 답변: 하루가 또 가네요 .\n",
      "START_TOKEN: [8168]\n",
      "END_TOKEN: [8169]\n",
      "VOCAB_SIZE: 8170\n",
      "Tokenized questions shape: (11823, 40)\n",
      "Dataset Input shape: (64, 39)\n",
      "Dataset Target shape: (64, 39)\n",
      "Sample Input: tf.Tensor(\n",
      "[8168 2078   80  541  148 3475 8169    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0], shape=(39,), dtype=int32)\n",
      "Sample Target: tf.Tensor(\n",
      "[2078   80  541  148 3475 8169    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0], shape=(39,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 데이터 로드\n",
    "path = os.getenv('HOME') + '/aiffel/songys_chatbot/ChatbotData.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# 전처리 함수 (동일)\n",
    "def preprocess_sentence(sentence):\n",
    "    # ... (기존 코드와 동일)\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 한국어, 영어, 숫자, 주요 구두점, 공백 외 제거 (기존 코드와 동일)\n",
    "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\s]\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# 질문, 답변 불러오기 (동일)\n",
    "def load_conversations():\n",
    "    inputs, outputs = [], []\n",
    "    for idx, row in data.iterrows():\n",
    "        q = row['Q']\n",
    "        a = row['A']\n",
    "        # 질문이나 답변이 문자열이 아닌 경우 건너뛰기 (데이터 오류 방지)\n",
    "        if isinstance(q, str) and isinstance(a, str):\n",
    "            inputs.append(preprocess_sentence(q))\n",
    "            outputs.append(preprocess_sentence(a))\n",
    "        elif isinstance(q, str): # 답변이 없거나 잘못된 경우 질문만 사용\n",
    "             inputs.append(preprocess_sentence(q))\n",
    "        elif isinstance(a, str): # 질문이 없거나 잘못된 경우 답변만 사용 (이 경우는 거의 없지만 안전하게)\n",
    "             outputs.append(preprocess_sentence(a))\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()\n",
    "print(f\"총 질문 수: {len(questions)}\")\n",
    "print(f\"총 답변 수: {len(answers)}\")\n",
    "print(\"전처리 후 샘플 질문:\", questions[0])\n",
    "print(\"전처리 후 샘플 답변:\", answers[0])\n",
    "\n",
    "\n",
    "# 토크나이저 빌드 (질문과 답변 모두 사용)\n",
    "# SubwordTextEncoder\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print(f\"START_TOKEN: {START_TOKEN}\")\n",
    "print(f\"END_TOKEN: {END_TOKEN}\")\n",
    "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "\n",
    "\n",
    "# 최대 문장 길이 (동일)\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 정수 인코딩 + 패딩 (질문 데이터만 사용하도록 수정)\n",
    "# 함수 이름은 유지하되, 내부 로직 변경 (GPT 언어 모델링에 맞게 질문만 처리)\n",
    "def tokenize_and_filter(inputs):\n",
    "    tokenized_inputs = []\n",
    "    for sentence in inputs:\n",
    "        # 시작 토큰과 종료 토큰 추가\n",
    "        sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "        # 최대 길이 이하인 경우만 list에 추가\n",
    "        if len(sentence) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence)\n",
    "\n",
    "    # 패딩 처리\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 질문 데이터만 토큰화 및 필터링\n",
    "questions_tokenized = tokenize_and_filter(questions)\n",
    "print(\"Tokenized questions shape:\", questions_tokenized.shape)\n",
    "\n",
    "\n",
    "# 데이터셋 준비 (GPT 언어 모델링 방식)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000 # 데이터가 작으므로 BUFFER_SIZE 조정 가능\n",
    "\n",
    "# 입력: questions_tokenized의 마지막 토큰 제외 [:, :-1]\n",
    "# 타겟: questions_tokenized의 첫 토큰 제외 [:, 1:]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    questions_tokenized[:, :-1], # 입력 시퀀스\n",
    "    questions_tokenized[:, 1:]  # 타겟 시퀀스 (입력을 한 스텝 민 것)\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 데이터셋 구조 확인 (하나의 배치 샘플 출력)\n",
    "for inputs, targets in dataset.take(1):\n",
    "    print(\"Dataset Input shape:\", inputs.shape)      # (BATCH_SIZE, MAX_LENGTH - 1)\n",
    "    print(\"Dataset Target shape:\", targets.shape)    # (BATCH_SIZE, MAX_LENGTH - 1)\n",
    "    print(\"Sample Input:\", inputs[0])\n",
    "    print(\"Sample Target:\", targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55213f7",
   "metadata": {},
   "source": [
    "*목차*\n",
    "1. Padding Mask\n",
    "2. Look Ahead Masking\n",
    "3. Multi Head Attention\n",
    "4. Decoding Layer & Decoder\n",
    "5. GPT Model define (word embedding + positional embedding)\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a1ddd",
   "metadata": {},
   "source": [
    "## Step 2. Masking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c78947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 입력 자체에 대한 패딩 마스크 생성\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231611ee",
   "metadata": {},
   "source": [
    "## Step 3. Multi-Head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7fff1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        scaled_attention = tf.matmul(attention_weights, value)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248f5d2",
   "metadata": {},
   "source": [
    "## Step 4. Decoder Layer (GPT-1)\n",
    "reference: Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a46f09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Decoder Layer 수정 (name 인자 추가)\n",
    "def decoder_layer(units, d_model, num_heads, dropout_rate, name=\"decoder_layer\"): # name 인자 추가\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"layer_inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"layer_look_ahead_mask\")\n",
    "\n",
    "    # Masked Multi-Head Self-Attention (첫 번째 서브층)\n",
    "    # MultiHeadAttention 레이어 이름도 고유하게 설정 (선택 사항)\n",
    "    attention_layer = MultiHeadAttention(d_model, num_heads) # 이름 중복 피하기 위해 인스턴스 생성\n",
    "    attention = attention_layer(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': look_ahead_mask # Look-ahead mask 적용\n",
    "    })\n",
    "    attention = tf.keras.layers.Dropout(dropout_rate)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + inputs) # Add & Norm\n",
    "\n",
    "    # Position-wise Feed-Forward Network (두 번째 서브층)\n",
    "    # 활성화 함수를 'relu' -> 'gelu'로 변경\n",
    "    outputs = tf.keras.layers.Dense(units, activation='gelu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention) # Add & Norm\n",
    "\n",
    "    # Model 생성 시 name 전달\n",
    "    return tf.keras.Model(inputs=[inputs, look_ahead_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfad02e",
   "metadata": {},
   "source": [
    "## Step 5. To define GPT model \n",
    "1. 텍스트를 숫자로 변환 후 임베딩\n",
    "2. 단어 위치 정보를 학습 가능한 벡터로 변환\n",
    "3. 단어의미 + 위치정보를 더하여 최종 입력 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03c39ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. GPT 모델 정의 수정 (decoder_layer 호출 시 고유 이름 전달)\n",
    "def gpt(vocab_size, num_layers, units, d_model, num_heads, dropout_rate, maximum_position_encoding):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(inputs)\n",
    "\n",
    "    token_embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) #Word Embedding\n",
    "\n",
    "    seq_len = tf.shape(inputs)[1]\n",
    "    positions = tf.range(start=0, limit=seq_len, delta=1) #Position Embedding\n",
    "    position_embeddings = tf.keras.layers.Embedding(maximum_position_encoding, d_model)(positions) #Position Embedding\n",
    "    position_embeddings = tf.expand_dims(position_embeddings, 0) #Position Embedding\n",
    "                                                                \n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings #Word Embedding + Position Embedding\n",
    "    embeddings = tf.keras.layers.Dropout(dropout_rate)(embeddings)\n",
    "\n",
    "    x = embeddings\n",
    "    for i in range(num_layers):\n",
    "        # decoder_layer 호출 시 고유한 이름(f\"decoder_layer_{i}\") 전달\n",
    "        x = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=f\"decoder_layer_{i}\" # 고유 이름 전달\n",
    "        )([x, look_ahead_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, name='outputs')(x)\n",
    "\n",
    "    # Model 생성 시 inputs와 outputs가 Tensor여야 함\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='GPT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d417e24",
   "metadata": {},
   "source": [
    "## Step 6. Loss Functions and Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277cb6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GPT\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_1 (TFOpLambd (2,)                 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli ()                   0           tf.compat.v1.shape_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.range_1 (TFOpLambda)         (None,)              0           tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 256)          10240       tf.range_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2091520     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (1, None, 256)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, None, 256)    0           embedding_2[0][0]                \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 256)    0           tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_0 (Functional)    (None, None, 256)    527104      dropout_5[0][0]                  \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_1 (Functional)    (None, None, 256)    527104      decoder_layer_0[0][0]            \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8170)   2099690     decoder_layer_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 5,255,658\n",
      "Trainable params: 5,255,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 6. Loss Functions and Model Compile (수정 없음 - 단, 모델 생성 시 파라미터 확인)\n",
    "\n",
    "# CustomSchedule, loss_function, accuracy 함수는 기존 코드와 동일\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # ... (기존 코드와 동일)\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) # step을 float32로 변환\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    # y_true shape: (batch_size, seq_len - 1)\n",
    "    # y_pred shape: (batch_size, seq_len - 1, vocab_size)\n",
    "    # 패딩된 부분(0)은 손실 계산에서 제외하기 위한 마스크 생성\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "\n",
    "    # SparseCategoricalCrossentropy 사용 (from_logits=True)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    # 마스크 적용\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    # 배치 내 평균 손실 계산 (마스크된 부분 제외하고 평균)\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    # return tf.reduce_mean(loss) # 기존 방식: 패딩 고려 안 된 평균일 수 있음\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # y_true shape: (batch_size, seq_len - 1)\n",
    "    # y_pred shape: (batch_size, seq_len - 1, vocab_size)\n",
    "    # 가장 확률 높은 토큰 예측\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    # 예측과 실제값이 같은지 확인\n",
    "    match = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "\n",
    "    # 패딩 마스크 생성\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "\n",
    "    # 마스크 적용\n",
    "    match = tf.multiply(match, mask)\n",
    "\n",
    "    # 정확도 계산 (마스크된 부분 제외하고 평균)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "    # return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) # 기존 방식: 패딩 고려 안 된 정확도\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 (GPT-1 논문과 다름, 실험용으로 작은 값 유지)\n",
    "NUM_LAYERS = 2 \n",
    "D_MODEL = 256  \n",
    "NUM_HEADS = 8  \n",
    "UNITS = 512    \n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# 모델 생성\n",
    "# maximum_position_encoding은 패딩 포함 최대 길이 MAX_LENGTH 사용\n",
    "model = gpt(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    maximum_position_encoding=MAX_LENGTH) # Positional Encoding 최대 길이\n",
    "\n",
    "# 옵티마이저 및 컴파일 (Learning Rate Schedule은 Transformer 논문 방식 유지)\n",
    "# GPT-1 논문은 cosine schedule 사용 [cite: 96]\n",
    "learning_rate = CustomSchedule(d_model=D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "# 모델 요약 출력 (평가 기준 9)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290486eb",
   "metadata": {},
   "source": [
    "## Step 7. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c43f8367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start Training for 20 epochs ---\n",
      "Epoch 1/20\n",
      "185/185 [==============================] - 10s 35ms/step - loss: 8.6161 - accuracy: 0.0992\n",
      "Epoch 2/20\n",
      "185/185 [==============================] - 6s 35ms/step - loss: 7.3494 - accuracy: 0.1543\n",
      "Epoch 3/20\n",
      "185/185 [==============================] - 6s 35ms/step - loss: 6.6402 - accuracy: 0.1610\n",
      "Epoch 4/20\n",
      "185/185 [==============================] - 7s 35ms/step - loss: 6.2858 - accuracy: 0.1746\n",
      "Epoch 5/20\n",
      "185/185 [==============================] - 7s 35ms/step - loss: 5.9959 - accuracy: 0.1931\n",
      "Epoch 6/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 5.6992 - accuracy: 0.2100\n",
      "Epoch 7/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 5.3880 - accuracy: 0.2279\n",
      "Epoch 8/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 5.0568 - accuracy: 0.2524\n",
      "Epoch 9/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 4.7081 - accuracy: 0.2817\n",
      "Epoch 10/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 4.3464 - accuracy: 0.3152\n",
      "Epoch 11/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 3.9819 - accuracy: 0.3485\n",
      "Epoch 12/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 3.6295 - accuracy: 0.3860\n",
      "Epoch 13/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 3.3027 - accuracy: 0.4213\n",
      "Epoch 14/20\n",
      "185/185 [==============================] - 7s 35ms/step - loss: 3.0019 - accuracy: 0.4628\n",
      "Epoch 15/20\n",
      "185/185 [==============================] - 7s 35ms/step - loss: 2.7441 - accuracy: 0.5011\n",
      "Epoch 16/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 2.5347 - accuracy: 0.5414\n",
      "Epoch 17/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 2.3705 - accuracy: 0.5748\n",
      "Epoch 18/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 2.2468 - accuracy: 0.6017\n",
      "Epoch 19/20\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 2.1722 - accuracy: 0.6177\n",
      "Epoch 20/20\n",
      "185/185 [==============================] - 7s 35ms/step - loss: 2.1202 - accuracy: 0.6307\n",
      "\n",
      "Total training time for 20 epochs: 135.20 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Step 7. Training ---\n",
    "EPOCHS = 20 #  epoch 20번 실행 하기\n",
    "print(f\"\\n--- Start Training for {EPOCHS} epochs ---\")\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTotal training time for {EPOCHS} epochs: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 학습 결과 시각화\n",
    "if history and history.history:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"학습 기록이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd72841",
   "metadata": {},
   "source": [
    "## Step 8. To Predict What's Coming on the Next? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98fbd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Test ---\n",
      "\n",
      "--- Predicting for: '오늘 날씨 어때' ---\n",
      "1. Tokenized input: [8168, 76, 1264, 376]\n",
      "2. Padded input shape: (1, 39)\n",
      "3. Prediction output shape: (1, 39, 8170)\n",
      "4. Last real token index: 3\n",
      "   Logits shape for last token: (8170,)\n",
      "5. Predicted token ID (argmax): 2\n",
      "   Top 5 predictions:\n",
      "     - Rank 1: ID=2, Prob=0.9798, Token=' ?'\n",
      "     - Rank 2: ID=8169, Prob=0.0197, Token='<END>'\n",
      "     - Rank 3: ID=174, Prob=0.0002, Token='서'\n",
      "     - Rank 4: ID=342, Prob=0.0001, Token='만'\n",
      "     - Rank 5: ID=1056, Prob=0.0000, Token='가지'\n",
      "6. Decoded token (argmax result): ' ?'\n",
      "  -> 최종 예측 다음 단어: ' ?' (ID: 2)\n",
      "\n",
      "--- Predicting for: '영화 추천해줘' ---\n",
      "1. Tokenized input: [8168, 953, 3900]\n",
      "2. Padded input shape: (1, 39)\n",
      "3. Prediction output shape: (1, 39, 8170)\n",
      "4. Last real token index: 2\n",
      "   Logits shape for last token: (8170,)\n",
      "5. Predicted token ID (argmax): 8169\n",
      "   Top 5 predictions:\n",
      "     - Rank 1: ID=8169, Prob=0.9361, Token='<END>'\n",
      "     - Rank 2: ID=1, Prob=0.0593, Token=' .'\n",
      "     - Rank 3: ID=205, Prob=0.0011, Token='여'\n",
      "     - Rank 4: ID=12, Prob=0.0009, Token=' . '\n",
      "     - Rank 5: ID=52, Prob=0.0008, Token='서 '\n",
      "6. Decoded token (argmax result): '<END>'\n",
      "  -> 최종 예측 다음 단어: '<END>' (ID: 8169)\n",
      "\n",
      "--- Predicting for: '배고파' ---\n",
      "1. Tokenized input: [8168, 2312]\n",
      "2. Padded input shape: (1, 39)\n",
      "3. Prediction output shape: (1, 39, 8170)\n",
      "4. Last real token index: 1\n",
      "   Logits shape for last token: (8170,)\n",
      "5. Predicted token ID (argmax): 8169\n",
      "   Top 5 predictions:\n",
      "     - Rank 1: ID=8169, Prob=0.5525, Token='<END>'\n",
      "     - Rank 2: ID=52, Prob=0.3311, Token='서 '\n",
      "     - Rank 3: ID=27, Prob=0.0276, Token='가 '\n",
      "     - Rank 4: ID=7944, Prob=0.0272, Token=' '\n",
      "     - Rank 5: ID=15, Prob=0.0111, Token='도 '\n",
      "6. Decoded token (argmax result): '<END>'\n",
      "  -> 최종 예측 다음 단어: '<END>' (ID: 8169)\n",
      "\n",
      "--- Predicting for: '안녕' ---\n",
      "1. Tokenized input: [8168, 906]\n",
      "2. Padded input shape: (1, 39)\n",
      "3. Prediction output shape: (1, 39, 8170)\n",
      "4. Last real token index: 1\n",
      "   Logits shape for last token: (8170,)\n",
      "5. Predicted token ID (argmax): 171\n",
      "   Top 5 predictions:\n",
      "     - Rank 1: ID=171, Prob=0.2494, Token='하'\n",
      "     - Rank 2: ID=906, Prob=0.1599, Token='안녕'\n",
      "     - Rank 3: ID=13, Prob=0.1517, Token='을 '\n",
      "     - Rank 4: ID=174, Prob=0.0155, Token='서'\n",
      "     - Rank 5: ID=112, Prob=0.0147, Token='진짜 '\n",
      "6. Decoded token (argmax result): '하'\n",
      "  -> 최종 예측 다음 단어: '하' (ID: 171)\n",
      "\n",
      "--- 코드 실행 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 8. Prediction\n",
    "def predict_next_token(sentence):\n",
    "    print(f\"\\n--- Predicting for: '{sentence}' ---\")\n",
    "    if not sentence: return \"입력 문장이 비어있습니다.\", -1\n",
    "\n",
    "    # 1. 전처리 및 토큰화\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    tokenized_sentence = START_TOKEN + tokenizer.encode(sentence)\n",
    "    print(f\"1. Tokenized input: {tokenized_sentence}\")\n",
    "\n",
    "    # 입력 길이 제한\n",
    "    if len(tokenized_sentence) >= MAX_LENGTH:\n",
    "         tokenized_sentence = tokenized_sentence[:MAX_LENGTH-1]\n",
    "         print(f\"   (Input truncated to {len(tokenized_sentence)} tokens)\")\n",
    "\n",
    "    # 2. 모델 입력 형태로 변환 및 패딩\n",
    "    encoder_input = tf.expand_dims(tokenized_sentence, 0)\n",
    "    padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        encoder_input, maxlen=MAX_LENGTH - 1, padding='post'\n",
    "    )\n",
    "    print(f\"2. Padded input shape: {padded_input.shape}\")\n",
    "\n",
    "    # 3. 모델 예측\n",
    "    try:\n",
    "        predictions = model(padded_input, training=False)\n",
    "        print(f\"3. Prediction output shape: {predictions.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Model prediction failed: {e}\")\n",
    "        return \"모델 예측 중 오류 발생\", -1\n",
    "\n",
    "    # 4. 마지막 타임스텝의 로짓 가져오기\n",
    "    last_token_index = tf.shape(encoder_input)[1].numpy() - 1\n",
    "    if last_token_index < 0: last_token_index = 0\n",
    "    print(f\"4. Last real token index: {last_token_index}\")\n",
    "\n",
    "    pred_seq_len = tf.shape(predictions)[1]\n",
    "    if last_token_index >= pred_seq_len:\n",
    "        print(f\"   Warning: last_token_index ({last_token_index}) >= pred_seq_len ({pred_seq_len}). Adjusting index.\")\n",
    "        last_token_index = pred_seq_len - 1\n",
    "\n",
    "    last_token_logits = predictions[0, last_token_index, :]\n",
    "    print(f\"   Logits shape for last token: {last_token_logits.shape}\")\n",
    "\n",
    "    # 5. 가장 확률 높은 토큰 ID 찾기 + Top 5 확인\n",
    "    predicted_token_id = tf.argmax(last_token_logits).numpy()\n",
    "    print(f\"5. Predicted token ID (argmax): {predicted_token_id}\")\n",
    "\n",
    "    probabilities = tf.nn.softmax(last_token_logits).numpy()\n",
    "    top_k_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_k_probs = probabilities[top_k_indices]\n",
    "\n",
    "    print(\"   Top 5 predictions:\")\n",
    "    for i in range(len(top_k_indices)):\n",
    "        current_id = top_k_indices[i]\n",
    "        # START_TOKEN 또는 END_TOKEN ID인지 확인 후 디코딩\n",
    "        if current_id == START_TOKEN[0]:\n",
    "            token_word = \"<START>\"\n",
    "        elif current_id == END_TOKEN[0]:\n",
    "            token_word = \"<END>\"\n",
    "        elif current_id < tokenizer.vocab_size: # 유효한 범위 내 ID만 decode 호출\n",
    "             try:\n",
    "                 token_word = tokenizer.decode([current_id])\n",
    "             except ValueError:\n",
    "                 token_word = \"<DECODE_ERROR>\" # 혹시 모를 다른 decode 오류 대비\n",
    "        else:\n",
    "            token_word = \"<INVALID_ID>\" # 범위 밖 ID 처리\n",
    "\n",
    "        print(f\"     - Rank {i+1}: ID={current_id}, Prob={top_k_probs[i]:.4f}, Token='{token_word}'\")\n",
    "\n",
    "    # 6. 최종 예측된 토큰 ID -> 단어 변환 (Special Token 처리 포함)\n",
    "    if predicted_token_id == START_TOKEN[0]:\n",
    "        predicted_token = \"<START>\"\n",
    "    elif predicted_token_id == END_TOKEN[0]:\n",
    "        predicted_token = \"<END>\"\n",
    "    elif predicted_token_id < tokenizer.vocab_size:\n",
    "        try:\n",
    "            predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        except ValueError:\n",
    "             predicted_token = \"<DECODE_ERROR>\"\n",
    "    else:\n",
    "         predicted_token = \"<INVALID_ID>\"\n",
    "\n",
    "    print(f\"6. Decoded token (argmax result): '{predicted_token}'\")\n",
    "\n",
    "    return predicted_token, int(predicted_token_id) # ID는 정수형으로 반환\n",
    "\n",
    "# 예측 테스트 (수정된 함수 사용)\n",
    "print(\"\\n--- Prediction Test ---\")\n",
    "test_sentences = [\"오늘 날씨 어때\", \"영화 추천해줘\", \"배고파\", \"안녕\"]\n",
    "for sent in test_sentences:\n",
    "    pred_word, pred_id = predict_next_token(sent)\n",
    "    print(f\"  -> 최종 예측 다음 단어: '{pred_word}' (ID: {pred_id})\")\n",
    "\n",
    "print(\"\\n--- 코드 실행 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a995f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
